{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings, itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "# you can either install the mpltools library or just comment out the next two lines\n",
    "# (n.b. ggplot style is much more readable)\n",
    "from mpltools import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Read in data</h1>\n",
    "<p>We're going to use a pandas dataframe to read in the csv file, and do some basic data exploration. The dataset we'll be using is admissions data describing applicants GPA, GRE score, and rank in school (1-4 for Freshman through Senior).  We'll start by reading in the data, and then looking at a few different views of it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/grad.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Plot the data</h1>\n",
    "<p>We always want to look at all or a porition of the data to make sure that no data cleansing needs to occure.  Create some distributions to view the data.  Before you move on, try to ask yourself the following question:\n",
    "\n",
    "given the distributions I see in the data, what do I think a reasonable accuracy is?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, sharey=False)\n",
    "fig.suptitle(\"Admission vs all Independent Variables\")\n",
    "fig.set_size_inches(15,10)\n",
    "#Admission vs....\n",
    "#GRE figure\n",
    "ax[0,0].scatter(df.gre, df.admit, c=df.admit)\n",
    "ax[0,0].set_title(\"GRE\")\n",
    "ax[0,0].set_xlabel(\"GRE\")\n",
    "ax[0,0].set_ylabel(\"Admission\")\n",
    "#GPA figure\n",
    "ax[0, 1].scatter(df.gpa, df.admit, c=df.admit)\n",
    "ax[0, 1].set_title(\"GPA\")\n",
    "ax[0, 1].set_xlabel(\"GPA\")\n",
    "ax[0, 1].set_ylabel(\"Admission\")\n",
    "#Rank figure\n",
    "ax[0, 2].scatter(df[\"rank\"], df.admit, c=df.admit)\n",
    "ax[0, 2].set_title(\"Rank\")\n",
    "ax[0, 2].set_xlabel(\"Rank\")\n",
    "ax[0, 2].set_ylabel(\"Admission\")\n",
    "#Admission as color vs....\n",
    "#GRE figure\n",
    "ax[1,0].scatter(df.gre, df.gpa, c=df.admit)\n",
    "ax[1,0].set_title(\"GRE vs GPA\")\n",
    "ax[1,0].set_xlabel(\"GRE\")\n",
    "ax[1,0].set_ylabel(\"GPA\")\n",
    "#GPA figure\n",
    "ax[1, 1].scatter(df.gre, df[\"rank\"], c=df.admit)\n",
    "ax[1, 1].set_title(\"GRE vs Rank\")\n",
    "ax[1, 1].set_xlabel(\"GRE\")\n",
    "ax[1, 1].set_ylabel(\"Rank\")\n",
    "#Rank figure\n",
    "ax[1, 2].scatter(df.gpa, df[\"rank\"], c=df.admit)\n",
    "ax[1, 2].set_title(\"GPA vs Rank\")\n",
    "ax[1, 2].set_xlabel(\"GPA\")\n",
    "ax[1, 2].set_ylabel(\"Rank\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Build the model</h1>\n",
    "<p>Create train test splits for your data.  Use the training data to .</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# get data in np arrays\n",
    "X = df.as_matrix(columns=[\"gre\", \"gpa\", \"rank\"])\n",
    "y = df.as_matrix(columns=[\"admit\"])\n",
    "\n",
    "# perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# define and train logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Generate classification predictions</h2>\n",
    "<p>Use the models 'predict' function to create classification output on your test set.  Build a confusion matrix to express the performance of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#taken from \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "# Create prediction array.\n",
    "preds = logreg.predict(X_test)\n",
    "\n",
    "# Use the sklearn confusion matrix class to create a confusion matrix, call the variable \"cm\"\n",
    "# so it works with the last line of this cell\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "#Print the accuracy_score and f1 score\n",
    "print(\"Model accuracy {}, f1 score {}\".format(accuracy_score(y_test, preds), f1_score(y_test, preds)))\n",
    "\n",
    "# Defined in cell above\n",
    "plot_confusion_matrix(cm, [\"admited, rejected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Create ROC Curve</h2>\n",
    "<p>Use the \"predict_proba\" method for your model to give probability scores.  Generate and plot a ROC curve for variable thresholds</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create array of regression scores for both classes using the 'predict_proba' class\n",
    "probs = logreg.predict_proba(X_test)\n",
    "\n",
    "# n.b. this class outputs probabilities for both admission and rejection.  \n",
    "# Create an array of just the admission probabilities\n",
    "pos_prob = [x[1] for x in probs]\n",
    "\n",
    "# Use the roc_curve method to create false positive rates, true positive rates\n",
    "fpr, tpr, thresh = roc_curve(y_test, pos_prob)\n",
    "\n",
    "# Use these rates to generate an 'area under the curve score', print it\n",
    "r_auc = auc(fpr, tpr)\n",
    "print(\"Area under ROC curve is {}\".format(r_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve.\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % r_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Discussion Prompt</h1>\n",
    "<p>\n",
    "P1: How does the model performance match your expectation of how well it should perform?<br/>\n",
    "\n",
    "P2: The accuracy, f1 score, and AUC all paint different pictures of performance.  Why is this?<br/>\n",
    "\n",
    "P3: Based on the ROC curve, do you think the default decision boundary of .5 gives you the best performance?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
